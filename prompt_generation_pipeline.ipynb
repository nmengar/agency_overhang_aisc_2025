{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the environment"
      ],
      "metadata": {
        "id": "I6cy3zEf5mSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJlbb1q1-SmU",
        "outputId": "3c66104e-278a-40f9-d852-45da3fd4f781"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "VlFGbkHD5m-t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "requirements.txt"
      ],
      "metadata": {
        "id": "A5aS-XqmtNsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers\n",
        "%pip install langchain\n",
        "%pip install openai\n",
        "%pip install google-generativeai\n",
        "%pip install boto3\n",
        "%pip install anthropic\n",
        "%pip install langchain-google-genai\n",
        "%pip install langchain-huggingface\n",
        "%pip install langchain-openai\n",
        "%pip install langchain-anthropic\n",
        "%pip install langchain-groq\n",
        "%pip install pandas\n",
        "%pip install tensorflow-datasets"
      ],
      "metadata": {
        "id": "tdxMSsTxs3pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3c7549-8703-4532-fc71-49c776c68462"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (4.9.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.26.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.25.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.16.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.6.0)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2025.1.31)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-datasets) (25.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.67.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "VytnmEOLvR-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "m-_kOsFavTw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Smarter Models\""
      ],
      "metadata": {
        "id": "Hy3VGQ81te4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smarter_models = {\n",
        "  'gemini-2.0-flash': init_chat_model(model=\"google_genai:gemini-2.0-flash-thinking-exp-01-21\"),\n",
        "  # 'deepseek': init_chat_model(model=\"perplexity-ai/r1-1776\", model_provider=\"huggingface\"),\n",
        "  'claude-3.5-haiku': init_chat_model(model=\"anthropic:claude-3-5-haiku-20241022\"),\n",
        "  # 'o1-mini': init_chat_model(model=\"openai:o1-mini\"),\n",
        "}"
      ],
      "metadata": {
        "id": "7sb_mGactnnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "task_prompt = \"Write python code to determine if a number prime or not, consider all the edge cases in input\"\n",
        "system_prompt_generated_knowledge = \"You are a teacher to an AI LLM. For the following task, generate a prompt that contains knowledge needed to solve the problem without giving the answer and start directly with the prompt, Task:\\n\"\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(system_prompt_generated_knowledge),\n",
        "    HumanMessage(task_prompt),\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "wOr87HlgzznS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smarter_model_responses = {}\n",
        "for model in smarter_models:\n",
        "  smarter_model_responses[model] = smarter_models[model].invoke(messages)"
      ],
      "metadata": {
        "id": "Yu3Y-v8A3Zjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for response in smarter_model_responses:\n",
        "  print(response+':',smarter_model_responses[response].content,'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfIpjjlCED6V",
        "outputId": "bc1a4981-4db5-4963-fcc2-11b88a7e29fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gemini-2.0-flash: Task:\n",
            "Write Python code to define a function called `is_prime(number)` that takes an integer as input and returns `True` if the number is a prime number and `False` otherwise.  Remember that a prime number is a natural number greater than 1 that has exactly two distinct positive divisors: 1 and itself.  When writing your code, consider and handle potential edge cases for the input number.  Specifically, think about how your code should behave for negative numbers, zero, and one, as these are not typically considered prime.  Also, think about the efficiency of your approach, especially when dealing with potentially large input numbers. Your function should be well-documented, explaining its purpose and how it handles different inputs. \n",
            "\n",
            "claude-3.5-haiku: Prompt: Create a Python function that determines whether a given integer is prime or not. Consider the following requirements:\n",
            "- Handle different types of input (integers, floats, strings)\n",
            "- Account for edge cases such as:\n",
            "  * Negative numbers\n",
            "  * Zero\n",
            "  * One\n",
            "  * Very large numbers\n",
            "- Provide efficient prime checking logic\n",
            "- Include input validation\n",
            "- Consider performance optimization techniques for prime number detection\n",
            "- Ensure the function returns a clear boolean result\n",
            "- Handle potential type conversion or input error scenarios\n",
            "\n",
            "Key points to think about:\n",
            "- What defines a prime number?\n",
            "- How can you optimize the prime checking algorithm?\n",
            "- What kinds of input validation are necessary?\n",
            "- How can you make the function robust and handle unexpected inputs? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark Datasets\n",
        "\n",
        "References\n",
        "\n",
        "(1) https://github.com/openai/evals/blob/main/examples/mmlu.ipynb"
      ],
      "metadata": {
        "id": "k2cemXdKviLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/AI\\ Safety\\ Camp/datasets/mmlu/\n",
        "# !pwd\n",
        "# !curl -O https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
        "# !tar -xf data.tar\n",
        "data_path = \"/content/drive/MyDrive/AI Safety Camp/datasets/mmlu/\""
      ],
      "metadata": {
        "id": "Yr-WcmUlvkxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7e587882-c86d-4e58-9a1f-8eaf143ad10e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AI Safety Camp/datasets/mmlu\n",
            "/content/drive/MyDrive/AI Safety Camp/datasets/mmlu\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  158M  100  158M    0     0  28.4M      0  0:00:05  0:00:05 --:--:-- 32.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# Construct a tf.data.Dataset\n",
        "ds = tfds.load('mnist', split='train', as_supervised=True, shuffle_files=True)\n",
        "\n",
        "# Build your input pipeline\n",
        "ds = ds.shuffle(1000).batch(128).prefetch(10).take(5)\n",
        "for image, label in ds:\n",
        "  pass"
      ],
      "metadata": {
        "id": "YaXTKbShD9Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-S63mW_9D870"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Generation and Storage Loop\n",
        "\n"
      ],
      "metadata": {
        "id": "ER279s9avmou"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRqwnf8fvpey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(TBD) Prompt Library Database (maybe RAG?)"
      ],
      "metadata": {
        "id": "awG0ev3tvqaX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "epFHs9xpwFBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Weaker\" Models"
      ],
      "metadata": {
        "id": "HlP2zodqwFp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weaker_models = {\n",
        "  'gemini-1.5-flash-8b': init_chat_model(model=\"google_genai:gemini-1.5-flash-8b\"),\n",
        "  'gemma2-9b-it': init_chat_model(model=\"groq:gemma2-9b-it\"),\n",
        "}\n",
        "\n",
        "weaker_model_messages = [\n",
        "    SystemMessage(\"You are a coding assistant. For the following task, you have to generate solution python code and nothing else. Task:\\n\"),\n",
        "    HumanMessage(task_prompt),\n",
        "]"
      ],
      "metadata": {
        "id": "3De3uQx1wIyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Weaker Model performance on benchmarks without scaffolding prompts"
      ],
      "metadata": {
        "id": "YM7FkK53wJMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weaker_model_responses = {}\n",
        "for model in weaker_models:\n",
        "  weaker_model_responses[model] = weaker_models[model].invoke(weaker_model_messages)"
      ],
      "metadata": {
        "id": "FaLPfw9HwQHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for response in weaker_model_responses:\n",
        "  print(response+':',weaker_model_responses[response].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq70iY2VEc_s",
        "outputId": "fff961fd-111b-4499-83d4-e874e6c144e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gemini-1.5-flash-8b: ```python\n",
            "import math\n",
            "\n",
            "def is_prime(number):\n",
            "    \"\"\"\n",
            "    Checks if a number is prime.\n",
            "\n",
            "    Args:\n",
            "        number: The number to check.\n",
            "\n",
            "    Returns:\n",
            "        True if the number is prime, False otherwise.  Returns False for invalid input.\n",
            "    \"\"\"\n",
            "    \n",
            "    #Handle invalid input\n",
            "    if not isinstance(number, int) or number <= 1:\n",
            "        return False\n",
            "    \n",
            "    if number <= 3:\n",
            "        return True\n",
            "    \n",
            "    if number % 2 == 0 or number % 3 == 0:\n",
            "        return False\n",
            "    \n",
            "    i = 5\n",
            "    while i * i <= number:\n",
            "        if number % i == 0 or number % (i + 2) == 0:\n",
            "            return False\n",
            "        i += 6\n",
            "    \n",
            "    return True\n",
            "```\n",
            "gemma2-9b-it: ```python\n",
            "def is_prime(number):\n",
            "  if number <= 1:\n",
            "    return False\n",
            "  if number <= 3:\n",
            "    return True\n",
            "  if number % 2 == 0 or number % 3 == 0:\n",
            "    return False\n",
            "  i = 5\n",
            "  while i * i <= number:\n",
            "    if number % i == 0 or number % (i + 2) == 0:\n",
            "      return False\n",
            "    i += 6\n",
            "  return True \n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Weaker Model performance on benchmarks with scaffolding prompt"
      ],
      "metadata": {
        "id": "Aa-Z4eYUwQyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaffolded_messages = []\n",
        "for model in smarter_model_responses:\n",
        "  scaffolded_messages.append([\n",
        "      SystemMessage(\"You are a coding assistant. For the following task, you have to generate solution python code and nothing else. Task:\\n\"),\n",
        "      SystemMessage(smarter_model_responses[model].content),\n",
        "      HumanMessage(task_prompt)\n",
        "  ])\n",
        "\n",
        "weaker_model_with_scaffolding_responses = {}\n",
        "for model in weaker_models:\n",
        "  for message in scaffolded_messages:\n",
        "    weaker_model_with_scaffolding_responses[model] = weaker_models[model].invoke(message)"
      ],
      "metadata": {
        "id": "iqCI55AQwWTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for response in weaker_model_with_scaffolding_responses:\n",
        "  print(response+':',weaker_model_with_scaffolding_responses[response].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mem3p8hrEgai",
        "outputId": "e01f1653-5511-468f-aabb-27116e75f953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gemini-1.5-flash-8b: ```python\n",
            "import math\n",
            "\n",
            "def is_prime(num):\n",
            "    \"\"\"\n",
            "    Efficiently determines if a given number is prime.\n",
            "\n",
            "    Args:\n",
            "        num: The number to check.\n",
            "\n",
            "    Returns:\n",
            "        True if the number is prime, False otherwise.  Returns False for invalid input.\n",
            "    \"\"\"\n",
            "\n",
            "    # Input validation\n",
            "    if not isinstance(num, (int, float)):\n",
            "        print(\"Error: Input must be an integer or a float.\")\n",
            "        return False\n",
            "    \n",
            "    num = int(num)  # Crucial for handling potential float imprecision\n",
            "    \n",
            "    if num <= 1:\n",
            "        return False\n",
            "    elif num <= 3:\n",
            "        return True\n",
            "    elif num % 2 == 0 or num % 3 == 0:\n",
            "        return False\n",
            "    \n",
            "    # Optimized primality test: Check divisibility up to the square root\n",
            "    i = 5\n",
            "    while i * i <= num:\n",
            "        if num % i == 0 or num % (i + 2) == 0:\n",
            "            return False\n",
            "        i += 6\n",
            "    \n",
            "    return True\n",
            "```\n",
            "gemma2-9b-it: ```python\n",
            "def is_prime(num):\n",
            "  \"\"\"\n",
            "  Determines if a given number is prime.\n",
            "\n",
            "  Args:\n",
            "    num: The number to check.\n",
            "\n",
            "  Returns:\n",
            "    True if the number is prime, False otherwise.\n",
            "  \"\"\"\n",
            "  if not isinstance(num, (int, float)):\n",
            "    raise TypeError(\"Input must be an integer or float.\")\n",
            "  num = int(num)\n",
            "  if num <= 1:\n",
            "    return False\n",
            "  if num <= 3:\n",
            "    return True\n",
            "  if num % 2 == 0 or num % 3 == 0:\n",
            "    return False\n",
            "  i = 5\n",
            "  while i * i <= num:\n",
            "    if num % i == 0 or num % (i + 2) == 0:\n",
            "      return False\n",
            "    i += 6\n",
            "  return True\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing and plotting change in performance"
      ],
      "metadata": {
        "id": "j-pte0d7wW6V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8IiGGYHwZqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}